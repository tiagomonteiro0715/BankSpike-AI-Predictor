{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c2ebc1-d85b-4642-a7e9-9c7241801364",
   "metadata": {},
   "source": [
    "## Binary classification of bank marketing data using Spiking Neural Networks with temporal dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e6693d-c5e8-40b5-86ef-9d10a06f4afe",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261c53cc-b289-46ca-baa8-bb8c163cb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiago\\Projects\\Advanced_AI_Research\\spiking_nn_projects\\env_one\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import snntorch as snn\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19164f3a-fc65-4159-a732-cc0123e042fd",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db00765c-351e-4f7a-a625-559727849656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age           job  marital  education default  balance housing loan  \\\n",
      "0   58    management  married   tertiary      no     2143     yes   no   \n",
      "1   44    technician   single  secondary      no       29     yes   no   \n",
      "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
      "3   47   blue-collar  married        NaN      no     1506     yes   no   \n",
      "4   33           NaN   single        NaN      no        1      no   no   \n",
      "\n",
      "  contact  day_of_week month  duration  campaign  pdays  previous poutcome  \n",
      "0     NaN            5   may       261         1     -1         0      NaN  \n",
      "1     NaN            5   may       151         1     -1         0      NaN  \n",
      "2     NaN            5   may        76         1     -1         0      NaN  \n",
      "3     NaN            5   may        92         1     -1         0      NaN  \n",
      "4     NaN            5   may       198         1     -1         0      NaN  \n",
      "    y\n",
      "0  no\n",
      "1  no\n",
      "2  no\n",
      "3  no\n",
      "4  no\n",
      "\n",
      "Column names: ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'day_of_week', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome']\n",
      "Data types:\n",
      " age             int64\n",
      "job            object\n",
      "marital        object\n",
      "education      object\n",
      "default        object\n",
      "balance         int64\n",
      "housing        object\n",
      "loan           object\n",
      "contact        object\n",
      "day_of_week     int64\n",
      "month          object\n",
      "duration        int64\n",
      "campaign        int64\n",
      "pdays           int64\n",
      "previous        int64\n",
      "poutcome       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "bank_marketing = fetch_ucirepo(id=222) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = bank_marketing.data.features \n",
    "y = bank_marketing.data.targets \n",
    "  \n",
    "print(X.head())\n",
    "print(y.head())\n",
    "print(\"\\nColumn names:\", X.columns.tolist())\n",
    "print(\"Data types:\\n\", X.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d99400-6abe-415d-8afd-239c0bcf7091",
   "metadata": {},
   "source": [
    "#### Identify categorical and numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aba7929-eff1-4dbe-b983-4f419447daf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categorical columns: ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
      "Numerical columns: ['age', 'balance', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous']\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64']).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorical columns: {categorical_cols}\")\n",
    "print(f\"Numerical columns: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c078f-7f1c-41f1-886c-a5e70f582bfe",
   "metadata": {},
   "source": [
    "#### Creating column transformer to pre process all collumns and a standart way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f6539-63d9-4194-850d-586efa88aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = ColumnTransformer(\n",
    "    [(\"one-hot\", OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),\n",
    "     (\"scaler\", StandardScaler(), numerical_cols)],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c0c4f-ea3f-41f6-872c-2b97eec7d1e9",
   "metadata": {},
   "source": [
    "### Applying transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca3736d-24dd-4f8e-bc02-d81132651bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ... -0.56935064 -0.41145311\n",
      "  -0.25194037]\n",
      " [ 0.          0.          0.         ... -0.56935064 -0.41145311\n",
      "  -0.25194037]\n",
      " [ 0.          0.          1.         ... -0.56935064 -0.41145311\n",
      "  -0.25194037]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.72181052  1.43618859\n",
      "   1.05047333]\n",
      " [ 0.          1.          0.         ...  0.39902023 -0.41145311\n",
      "  -0.25194037]\n",
      " [ 0.          0.          1.         ... -0.24656035  1.4761376\n",
      "   4.52357654]]\n",
      "[0 0 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Transform X\n",
    "X_transformed = column_transformer.fit_transform(X)\n",
    "\n",
    "# Transform y\n",
    "label_encoder = LabelEncoder()\n",
    "y_transformed = label_encoder.fit_transform(y.values.ravel())\n",
    "\n",
    "print(X_transformed)\n",
    "print(y_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725315ce-c17c-4061-90d2-304db18a861f",
   "metadata": {},
   "source": [
    "### Export preprocessing parameters to JSON for production use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32748efa-4c84-4cd5-be87-5b62c72c6438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Preprocessing parameters saved to: preprocessing_params_20251015_111305.json\n",
      "  - OneHotEncoder categories: 9 categorical features\n",
      "  - StandardScaler: 7 numerical features\n"
     ]
    }
   ],
   "source": [
    "preprocessing_params = {\n",
    "    \"column_transformer_info\": {\n",
    "        \"categorical_columns\": categorical_cols,\n",
    "        \"numerical_columns\": numerical_cols\n",
    "    },\n",
    "    \"one_hot_encoder\": {\n",
    "        \"categories\": [cat.tolist() for cat in column_transformer.named_transformers_['one-hot'].categories_],\n",
    "        \"feature_names\": column_transformer.named_transformers_['one-hot'].get_feature_names_out(categorical_cols).tolist(),\n",
    "        \"handle_unknown\": \"ignore\"\n",
    "    },\n",
    "    \"standard_scaler\": {\n",
    "        \"mean\": column_transformer.named_transformers_['scaler'].mean_.tolist(),\n",
    "        \"scale\": column_transformer.named_transformers_['scaler'].scale_.tolist(),\n",
    "        \"var\": column_transformer.named_transformers_['scaler'].var_.tolist(),\n",
    "        \"feature_names\": numerical_cols\n",
    "    },\n",
    "    \"label_encoder\": {\n",
    "        \"classes\": label_encoder.classes_.tolist(),\n",
    "        \"class_mapping\": {str(cls): int(idx) for idx, cls in enumerate(label_encoder.classes_)}\n",
    "    },\n",
    "    \"input_dimension\": int(X_transformed.shape[1]),\n",
    "    \"preprocessing_date\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "preprocessing_filename = f\"preprocessing_params_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(preprocessing_filename, 'w') as f:\n",
    "    json.dump(preprocessing_params, f, indent=4)\n",
    "\n",
    "print(f\"\\n Preprocessing parameters saved to: {preprocessing_filename}\")\n",
    "print(f\"  - OneHotEncoder categories: {len(preprocessing_params['one_hot_encoder']['categories'])} categorical features\")\n",
    "print(f\"  - StandardScaler: {len(preprocessing_params['standard_scaler']['mean'])} numerical features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898d8edb-380d-4287-99c5-fa5156c95a1e",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b4d65d2-2f15-4f5e-9c3f-cce56a19fb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27126, 51) (9042, 51) (9043, 51)\n",
      "(27126,) (9042,) (9043,)\n"
     ]
    }
   ],
   "source": [
    "# First split: separate out 60% for training, 40% for temp (dev + test)\n",
    "# Stratify keeps the same ratio of \"yes\" and \"no\" in training, dev, and test sets.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_transformed, y_transformed, \n",
    "    test_size=0.4,  # 40% for dev + test\n",
    "    random_state=42,  # for reproducibility\n",
    "    stratify=y_transformed  # maintain class distribution\n",
    ")\n",
    "\n",
    "# Second split: split the 40% into 20% dev and 20% test (50/50 split of the 40%)\n",
    "# Stratify keeps the same ratio of \"yes\" and \"no\" in training, dev, and test sets.\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,  # 50% of 40% = 20% of total\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_dev.shape, X_test.shape)\n",
    "print(y_train.shape, y_dev.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e783b-40fc-4c7f-a631-743607685b38",
   "metadata": {},
   "source": [
    "#### Convert data to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47567ad2-0536-43cd-8805-e8bb293ef58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_dev_tensor = torch.FloatTensor(X_dev)\n",
    "y_dev_tensor = torch.LongTensor(y_dev)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61cf394-57f8-4881-9aaa-aba199fad118",
   "metadata": {},
   "source": [
    "#### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e661cc77-c12d-46ad-8a19-6723a7a28f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dev_dataset = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2060977c-0c56-483a-8092-7ffb63da1a88",
   "metadata": {},
   "source": [
    "#### Defining Spiking Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11888895-042a-49eb-8955-74f2068681b8",
   "metadata": {},
   "source": [
    "#### Fixed Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178db633-4136-4ae4-b878-4eb3415e2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = X_train.shape[1]\n",
    "num_outputs = len(set(y_transformed)) # Number of unique classes (2: subscribed or not subscribed to term deposit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee2b5cf-9275-4632-8823-147484eb79c0",
   "metadata": {},
   "source": [
    "#### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a3519-a2ac-4536-9d2a-f3265d02dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, num_steps, epoch, num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = len(loader)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} - Training:\")\n",
    "    print(f\"  Processing {num_batches} batches...\")\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(loader):\n",
    "        mem1 = torch.zeros(data.size(0), num_hidden)\n",
    "        spk1 = torch.zeros(data.size(0), num_hidden)\n",
    "        mem2 = torch.zeros(data.size(0), num_outputs)\n",
    "        \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through time\n",
    "        spk2_rec = []  # Record output spikes\n",
    "        mem2_rec = []  # Record output membrane potentials\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            mem1, spk1, mem2, spk2 = model(data, mem1, spk1, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "        \n",
    "        # Stack recordings\n",
    "        spk2_rec = torch.stack(spk2_rec)  # [num_steps, batch_size, num_outputs]\n",
    "        mem2_rec = torch.stack(mem2_rec)\n",
    "        \n",
    "        # Loss calculation - use final membrane potential\n",
    "        loss = criterion(mem2_rec[-1], targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy using spike counts\n",
    "        spike_count = spk2_rec.sum(dim=0)  # Sum spikes over time\n",
    "        _, predicted = spike_count.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print batch progress every 10 batches or on last batch\n",
    "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == num_batches:\n",
    "            current_acc = 100 * correct / total\n",
    "            current_loss = total_loss / (batch_idx + 1)\n",
    "            print(f\"    Batch {batch_idx+1}/{num_batches} - Loss: {current_loss:.4f}, Acc: {current_acc:.2f}%\")\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_acc = 100 * correct / total\n",
    "    print(f\"  Epoch {epoch+1} Training Complete - Avg Loss: {avg_loss:.4f}, Avg Acc: {avg_acc:.2f}%\")\n",
    "    \n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6763c0c9-0309-46e0-9b12-c231b88304a0",
   "metadata": {},
   "source": [
    "#### Evaluating Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e95e2fb0-d999-47f6-b79d-8f92de4df952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, num_steps, phase=\"Validation\"):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(f\"  {phase}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in loader:\n",
    "            # Initialize membrane potentials and spikes\n",
    "            mem1 = torch.zeros(data.size(0), num_hidden)\n",
    "            spk1 = torch.zeros(data.size(0), num_hidden)\n",
    "            mem2 = torch.zeros(data.size(0), num_outputs)\n",
    "            \n",
    "            # Forward pass through time\n",
    "            spk2_rec = []\n",
    "            mem2_rec = []\n",
    "            \n",
    "            for step in range(num_steps):\n",
    "                mem1, spk1, mem2, spk2 = model(data, mem1, spk1, mem2)\n",
    "                spk2_rec.append(spk2)\n",
    "                mem2_rec.append(mem2)\n",
    "            \n",
    "            spk2_rec = torch.stack(spk2_rec)\n",
    "            mem2_rec = torch.stack(mem2_rec)\n",
    "            \n",
    "            # Loss\n",
    "            loss = criterion(mem2_rec[-1], targets)\n",
    "            \n",
    "            # Accuracy\n",
    "            spike_count = spk2_rec.sum(dim=0)\n",
    "            _, predicted = spike_count.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_acc = 100 * correct / total\n",
    "    print(f\"  {phase} Complete - Loss: {avg_loss:.4f}, Acc: {avg_acc:.2f}%\")\n",
    "    \n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c646709f-09d8-4151-940d-af9bd9edd20a",
   "metadata": {},
   "source": [
    "## Define the SNN model class with trial parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c43bcf7-4087-4f2a-971e-62dd1a5e96a7",
   "metadata": {},
   "source": [
    "### AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "805858c4-d51c-465e-8db2-a8316d1ba649",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_sizes, num_outputs, beta, threshold):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create layers dynamically based on hidden_sizes list\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.lif_layers = nn.ModuleList()\n",
    "        \n",
    "        # Input to first hidden layer\n",
    "        prev_size = num_inputs\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            self.lif_layers.append(snn.Leaky(beta=beta, threshold=threshold))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Last hidden to output layer\n",
    "        self.fc_out = nn.Linear(prev_size, num_outputs)\n",
    "        self.lif_out = snn.Leaky(beta=beta, threshold=threshold)\n",
    "\n",
    "    def forward(self, x, mem_states, spk_states):\n",
    "        \"\"\"\n",
    "        mem_states and spk_states should be lists with one element per layer\n",
    "        \"\"\"\n",
    "        new_mem_states = []\n",
    "        new_spk_states = []\n",
    "        \n",
    "        current_input = x\n",
    "        \n",
    "        # Process through hidden layers\n",
    "        for i, (fc, lif) in enumerate(zip(self.layers, self.lif_layers)):\n",
    "            cur = fc(current_input)\n",
    "            spk, mem = lif(cur, mem_states[i])\n",
    "            new_mem_states.append(mem)\n",
    "            new_spk_states.append(spk)\n",
    "            current_input = spk  # Next layer's input is current layer's spike\n",
    "        \n",
    "        # Output layer\n",
    "        cur_out = self.fc_out(current_input)\n",
    "        spk_out, mem_out = self.lif_out(cur_out, mem_states[-1])\n",
    "        new_mem_states.append(mem_out)\n",
    "        new_spk_states.append(spk_out)\n",
    "        \n",
    "        return new_mem_states, new_spk_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec49df9-3ea4-44f2-a848-918a8d376100",
   "metadata": {},
   "source": [
    "### Training function for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91c477d9-ea46-46fd-9e64-d0275062c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, num_steps, hidden_sizes, num_outputs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data, targets in loader:\n",
    "        # Initialize membrane potentials and spikes for all layers\n",
    "        mem_states = [torch.zeros(data.size(0), size) for size in hidden_sizes]\n",
    "        mem_states.append(torch.zeros(data.size(0), num_outputs))  # Output layer\n",
    "        \n",
    "        spk_states = [torch.zeros(data.size(0), size) for size in hidden_sizes]\n",
    "        spk_states.append(torch.zeros(data.size(0), num_outputs))  # Output layer\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        spk_out_rec = []\n",
    "        mem_out_rec = []\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            mem_states, spk_states = model(data, mem_states, spk_states)\n",
    "            spk_out_rec.append(spk_states[-1])  # Last layer spikes\n",
    "            mem_out_rec.append(mem_states[-1])  # Last layer membrane\n",
    "        \n",
    "        spk_out_rec = torch.stack(spk_out_rec)\n",
    "        mem_out_rec = torch.stack(mem_out_rec)\n",
    "        \n",
    "        loss = criterion(mem_out_rec[-1], targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        spike_count = spk_out_rec.sum(dim=0)\n",
    "        _, predicted = spike_count.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a421be58-da4a-426e-9cf9-c3e458ba77c0",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1516d408-ba72-4479-bda4-05a03afcd045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, num_steps, hidden_sizes, num_outputs):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in loader:\n",
    "            # Initialize states for all layers\n",
    "            mem_states = [torch.zeros(data.size(0), size) for size in hidden_sizes]\n",
    "            mem_states.append(torch.zeros(data.size(0), num_outputs))\n",
    "            \n",
    "            spk_states = [torch.zeros(data.size(0), size) for size in hidden_sizes]\n",
    "            spk_states.append(torch.zeros(data.size(0), num_outputs))\n",
    "            \n",
    "            spk_out_rec = []\n",
    "            mem_out_rec = []\n",
    "            \n",
    "            for step in range(num_steps):\n",
    "                mem_states, spk_states = model(data, mem_states, spk_states)\n",
    "                spk_out_rec.append(spk_states[-1])\n",
    "                mem_out_rec.append(mem_states[-1])\n",
    "            \n",
    "            spk_out_rec = torch.stack(spk_out_rec)\n",
    "            mem_out_rec = torch.stack(mem_out_rec)\n",
    "            \n",
    "            loss = criterion(mem_out_rec[-1], targets)\n",
    "            \n",
    "            spike_count = spk_out_rec.sum(dim=0)\n",
    "            _, predicted = spike_count.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0447d-6256-4eb2-92a6-f628fe03dc5f",
   "metadata": {},
   "source": [
    "### Optuna objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004cf402-2153-4112-81fc-f0b5241d023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Most important hyperparameter: learning rate\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-2, log=True)\n",
    "    \n",
    "    # Adam momentum parameters\n",
    "    beta1 = trial.suggest_float('adam_beta1', 0.85, 0.98)\n",
    "    beta2 = trial.suggest_float('adam_beta2', 0.95, 0.9999)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    \n",
    "    # Network architecture hyperparameters\n",
    "    hidden_sizes = trial.suggest_categorical('hidden_sizes', [\n",
    "        [64], [128], [256],    \n",
    "        [256, 128], [256, 256], [128, 128], [128, 64], [64, 64], [256, 64], [128, 32],\n",
    "        [256, 256, 128], [256, 128, 64], [256, 128, 128], [128, 128, 128], [128, 128, 64],\n",
    "        [128, 64, 64], [128, 64, 32], [64, 64, 64], [256, 256, 256],\n",
    "        [256, 256, 128, 64], [256, 128, 128, 64], [256, 128, 64, 64], [256, 128, 64, 32], [128, 128, 128, 64], \n",
    "        [128, 128, 64, 64], [128, 128, 64, 32], [128, 64, 64, 32], [64, 64, 64, 64], [256, 256, 256, 128], \n",
    "        [256, 256, 128, 128, 64], [256, 256, 128, 64, 64], [256, 256, 128, 64, 32], [256, 128, 128, 64, 64],\n",
    "        [256, 128, 128, 64, 32], [256, 128, 64, 64, 32], [256, 128, 64, 32, 32], [128, 128, 128, 64, 64], \n",
    "        [128, 128, 64, 64, 64], [128, 128, 64, 64, 32], [128, 64, 64, 64, 32], [64, 64, 64, 64, 64],\n",
    "        [256, 256, 256, 128, 64], [256, 256, 256, 256, 128],\n",
    "    ])\n",
    "    \n",
    "    # Learning rate decay parameters\n",
    "    lr_patience = trial.suggest_int('lr_patience', 2, 5)\n",
    "    lr_factor = trial.suggest_float('lr_factor', 0.1, 0.5)\n",
    "\n",
    "    # Spiking Neural Network Specific Hyperparameters\n",
    "    beta = trial.suggest_float('beta', 0.3, 0.9999)\n",
    "    threshold = trial.suggest_float('threshold', 0.8, 15.0)\n",
    "    num_steps = trial.suggest_int('num_steps', 2, 20, step=5)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    dev_dataset = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model with hidden_sizes\n",
    "    model = Net(num_inputs, hidden_sizes, num_outputs, beta, threshold)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=learning_rate,\n",
    "        betas=(beta1, beta2)\n",
    "    )    \n",
    "\n",
    "    # learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max', \n",
    "        factor=lr_factor, \n",
    "        patience=lr_patience\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 10 \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, num_steps, hidden_sizes, num_outputs\n",
    "        )\n",
    "        dev_loss, dev_acc = evaluate(\n",
    "            model, dev_loader, criterion, num_steps, hidden_sizes, num_outputs\n",
    "        )\n",
    "        \n",
    "        # Report intermediate value\n",
    "        trial.report(dev_acc, epoch)\n",
    "        \n",
    "        # Handle pruning\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return dev_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa2d0a-3a07-46ac-be87-5864ebb6617f",
   "metadata": {},
   "source": [
    "### Run Optuna optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4179a3bf-7522-43e7-9a81-7c387a88c907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-15 11:13:06,040] A new study created in memory with name: no-name-2114df11-cfe2-47e3-a3a5-168f61884d90\n"
     ]
    }
   ],
   "source": [
    "# Create study\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a93af3-3c63-41db-959a-04b3ba206144",
   "metadata": {},
   "source": [
    "#### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f684305c-0e08-4775-bd5b-132b3e7253c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimization started at: 2025-10-15 11:13:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 88.3543:  33%|██████████████████████████████▋                                                             | 1/3 [03:01<06:03, 181.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-15 11:16:07,877] Trial 0 finished with value: 88.35434638354346 and parameters: {'learning_rate': 6.38362778772101e-05, 'adam_beta1': 0.8696679280772431, 'adam_beta2': 0.9980169341567128, 'batch_size': 32, 'hidden_sizes': [256], 'lr_patience': 3, 'lr_factor': 0.20267435957747423, 'beta': 0.47196584568104916, 'threshold': 1.4284481225138923, 'num_steps': 7}. Best is trial 0 with value: 88.35434638354346.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 88.3543:  67%|█████████████████████████████████████████████████████████████▎                              | 2/3 [04:33<02:08, 128.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-15 11:17:39,220] Trial 1 finished with value: 88.29904888299049 and parameters: {'learning_rate': 2.249070465739675e-05, 'adam_beta1': 0.9605343209469603, 'adam_beta2': 0.9930480949108201, 'batch_size': 256, 'hidden_sizes': [128, 64, 64, 64, 32], 'lr_patience': 3, 'lr_factor': 0.31079640637625705, 'beta': 0.7903996502338146, 'threshold': 5.54708369048015, 'num_steps': 7}. Best is trial 0 with value: 88.35434638354346.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 88.3543: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [09:18<00:00, 186.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-15 11:22:24,104] Trial 2 finished with value: 88.29904888299049 and parameters: {'learning_rate': 1.5045718200634862e-06, 'adam_beta1': 0.9759209474591165, 'adam_beta2': 0.9877004283788553, 'batch_size': 128, 'hidden_sizes': [256, 128, 64, 32, 32], 'lr_patience': 4, 'lr_factor': 0.15048177623861345, 'beta': 0.41168771817638805, 'threshold': 10.987706721801285, 'num_steps': 12}. Best is trial 0 with value: 88.35434638354346.\n",
      "\n",
      " Optimization ended at: 2025-10-15 11:22:24\n",
      " Total time elapsed: 558.06 seconds (9.30 minutes)\n",
      " Average time per trial: 186.02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "start_datetime = datetime.datetime.now()\n",
    "print(f\"\\n Optimization started at: {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "#We only do three trials to save time. However this code can be used with GPUs to increase the chances of finding the best hyperparameters\n",
    "study.optimize(objective, n_trials=3, timeout=None, show_progress_bar=True)\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "end_datetime = datetime.datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n Optimization ended at: {end_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\" Total time elapsed: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
    "\n",
    "print(f\" Average time per trial: {elapsed_time/len(study.trials):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c23fb-558b-45c7-8ea1-f932b4e82ddd",
   "metadata": {},
   "source": [
    "### Display best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "351c2d4c-af4e-428a-a4ba-91b92f5762d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best trial:\n",
      "  Value (Dev Accuracy): 88.35%\n",
      "\n",
      "Best hyperparameters:\n",
      "  learning_rate: 6.38362778772101e-05\n",
      "  adam_beta1: 0.8696679280772431\n",
      "  adam_beta2: 0.9980169341567128\n",
      "  batch_size: 32\n",
      "  hidden_sizes: [256]\n",
      "  lr_patience: 3\n",
      "  lr_factor: 0.20267435957747423\n",
      "  beta: 0.47196584568104916\n",
      "  threshold: 1.4284481225138923\n",
      "  num_steps: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBest trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  Value (Dev Accuracy): {trial.value:.2f}%\")\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "best_params = trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5405d9f5-15a1-4205-8bb9-dcb80cf73374",
   "metadata": {},
   "source": [
    "## Train final model with best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53daed44-1df7-4885-a397-2dab669d7fae",
   "metadata": {},
   "source": [
    "### Create data loaders with best batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38203731-f414-400c-bc35-ee57afc573ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "dev_dataset = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6a6dc1-7635-4d4e-bdd8-0f94d3429105",
   "metadata": {},
   "source": [
    "### Initialize final model and train him"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd1f3fea-ab90-43f4-ba04-caa39830fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(\n",
    "    num_inputs, \n",
    "    best_params['hidden_sizes'],  # ✓ Get it from the best parameters\n",
    "    num_outputs, \n",
    "    best_params['beta'], \n",
    "    best_params['threshold']\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5d7be2e-bfc0-43f6-a0a3-f1bdc0b479b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/8\n",
      "  Train - Loss: 0.4866, Acc: 88.30%\n",
      "  Dev   - Loss: 0.3064, Acc: 88.30%\n",
      "  New best model saved!\n",
      "\n",
      "Epoch 2/8\n",
      "  Train - Loss: 0.2823, Acc: 88.30%\n",
      "  Dev   - Loss: 0.2653, Acc: 88.30%\n",
      "\n",
      "Epoch 3/8\n",
      "  Train - Loss: 0.2520, Acc: 88.30%\n",
      "  Dev   - Loss: 0.2464, Acc: 88.31%\n",
      "  New best model saved!\n",
      "\n",
      "Epoch 4/8\n",
      "  Train - Loss: 0.2414, Acc: 88.31%\n",
      "  Dev   - Loss: 0.2396, Acc: 88.33%\n",
      "  New best model saved!\n",
      "\n",
      "Epoch 5/8\n",
      "  Train - Loss: 0.2341, Acc: 88.30%\n",
      "  Dev   - Loss: 0.2342, Acc: 88.33%\n",
      "\n",
      "Epoch 6/8\n",
      "  Train - Loss: 0.2303, Acc: 88.31%\n",
      "  Dev   - Loss: 0.2316, Acc: 88.33%\n",
      "\n",
      "Epoch 7/8\n",
      "  Train - Loss: 0.2272, Acc: 88.31%\n",
      "  Dev   - Loss: 0.2303, Acc: 88.32%\n",
      "\n",
      "Epoch 8/8\n",
      "  Train - Loss: 0.2238, Acc: 88.31%\n",
      "  Dev   - Loss: 0.2295, Acc: 88.32%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 8\n",
    "best_dev_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, \n",
    "        best_params['num_steps'], best_params['hidden_sizes'], num_outputs  # ✓ Use 'hidden_sizes'\n",
    "    )\n",
    "    dev_loss, dev_acc = evaluate(\n",
    "        model, dev_loader, criterion, \n",
    "        best_params['num_steps'], best_params['hidden_sizes'], num_outputs  # ✓ Use 'hidden_sizes'\n",
    "    )\n",
    "    \n",
    "    print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Dev   - Loss: {dev_loss:.4f}, Acc: {dev_acc:.2f}%\")\n",
    "    \n",
    "    if dev_acc > best_dev_acc:\n",
    "        best_dev_acc = dev_acc\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), 'best_snn_model_optuna.pth')\n",
    "        print(f\"  New best model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb57b06-574a-40f1-9f31-ffc806bffa0a",
   "metadata": {},
   "source": [
    "### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c16dda1-e3f2-4bbf-a082-cd4fa313ce01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results:\n",
      "  Best Dev Accuracy: 88.33% (epoch 4)\n",
      "  Test Accuracy: 88.30%\n",
      "  Test Loss: 0.2383\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_snn_model_optuna.pth'))\n",
    "test_loss, test_acc = evaluate(\n",
    "    model, test_loader, criterion, \n",
    "    best_params['num_steps'], best_params['hidden_sizes'], num_outputs  # ✓ Use 'hidden_sizes'\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Best Dev Accuracy: {best_dev_acc:.2f}% (epoch {best_epoch})\")\n",
    "print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff197989-7b8e-4a4e-b225-092cf7700b4b",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47dae389-0cfd-4326-b782-2920be38afc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Results saved to: optuna_results_20251015_113406.json\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"optimization_info\": {\n",
    "        \"n_trials\": len(study.trials),\n",
    "        \"best_trial_number\": study.best_trial.number,\n",
    "        \"optimization_date\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    },\n",
    "    \"best_hyperparameters\": best_params,\n",
    "    \"model_architecture\": {\n",
    "        \"num_inputs\": int(num_inputs),\n",
    "        \"hidden_sizes\": best_params['hidden_sizes'], \n",
    "        \"num_outputs\": int(num_outputs),\n",
    "        \"total_parameters\": int(sum(p.numel() for p in model.parameters()))  \n",
    "    },\n",
    "    \"final_results\": {\n",
    "        \"best_dev_accuracy\": float(best_dev_acc),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"test_accuracy\": float(test_acc),\n",
    "        \"test_loss\": float(test_loss)\n",
    "    },\n",
    "    \"data_info\": {\n",
    "        \"train_samples\": int(len(train_loader.dataset)),\n",
    "        \"dev_samples\": int(len(dev_loader.dataset)),\n",
    "        \"test_samples\": int(len(test_loader.dataset)),\n",
    "        \"dataset\": \"Bank Marketing (UCI ML Repository ID: 222)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "json_filename = f\"optuna_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(json_filename, 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "    \n",
    "print(f\"\\n Results saved to: {json_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8059463-4574-4b66-9841-254f7ff7285e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spiking NN env",
   "language": "python",
   "name": "env_one"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
